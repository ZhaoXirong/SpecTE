{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6866884-224d-4cfc-addb-7a2c9e0b443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d # 线性插值\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt # 中值滤波\n",
    "from sklearn.preprocessing import StandardScaler  # 数据进行标准化\n",
    "import json \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9dc90-42b4-4c21-97da-91c85001cbe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 函数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32b72-eb4f-4734-b300-d77fba5cb845",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 工具函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce779eb9-c12a-4d82-8baa-00a34d01b6ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 查看一个fit文件下有什么字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c5648d-b175-402b-bc72-2105540e947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_fits_headers(fits_path):\n",
    "    \"\"\"\n",
    "    列出压缩FITS文件中头文件的所有字段。\n",
    "    \n",
    "    参数：\n",
    "    fits_path (str): FITS文件的路径（支持.gz压缩文件）。\n",
    "    \n",
    "    返回：\n",
    "    headers (dict): FITS文件中头文件的所有字段及其值。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fits_path):\n",
    "        raise FileNotFoundError(f\"The file {fits_path} does not exist.\")\n",
    "    \n",
    "    headers = {}\n",
    "    \n",
    "    with fits.open(fits_path) as hdul:\n",
    "        for idx, hdu in enumerate(hdul):\n",
    "            header = hdu.header\n",
    "            headers[f\"HDU_{idx}\"] = dict(header.items())\n",
    "    \n",
    "    return headers\n",
    "\n",
    "# # 示例用法\n",
    "# fits_path = r'G:\\Star\\1_Data_download_and_preprocessing\\Denoising_reference_set\\download\\FITSDATA\\high_0_10\\spec-55863-B6301_sp03-211.fits.gz'\n",
    "# headers = list_fits_headers(fits_path)\n",
    "# for hdu_name, header in headers.items():\n",
    "#     print(f\"HDU Name: {hdu_name}\")\n",
    "#     for key, value in header.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129760ad-bccb-4741-ab2e-93a227c305de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 遍历目录all_path："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857890b-cf07-49b6-aee3-c8271f9733b8",
   "metadata": {},
   "source": [
    "功能描述\n",
    "\n",
    "    函数遍历指定目录（包括其子目录）下的所有文件，并返回一个包含所有文件名的列表。\n",
    "\n",
    "参数\n",
    "\n",
    "    dirname (str): 需要遍历的目录路径字符串。\n",
    "返回值\n",
    "\n",
    "    返回一个列表，包含遍历得到的所有文件的名称（不包括路径）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0b3eaa-c2e7-40d3-a159-cd8c0af0d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_path(dirname):\n",
    "    files_names = []\n",
    "    for maindir, subdir, file_name_list in os.walk(dirname):\n",
    "        for filename in file_name_list:\n",
    "            files_names.append(filename)\n",
    "    return files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b4171-b7ea-4b05-bfbc-f4e863f576f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.Wavelength Analysis 波长分析(wave_analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfccb-63e7-407d-8fb2-31387662e8dd",
   "metadata": {},
   "source": [
    "wave_analyse 函数用于分析给定目录下的 FITS 文件中记录的天体光谱数据，计算并返回给定数据集中天体光谱的蓝端和红端波长范围的最小和最大值，同时考虑了红移的影响。\n",
    "\n",
    "参数\n",
    "\n",
    "    DATASET_PATH (str): 指向包含光谱数据概览的 CSV 文件的路径。该文件用于辅助分析，但本函数示例中未直接使用该参数。\n",
    "    FITS_PATH (str): 包含 FITS 文件的目录路径。这些 FITS 文件包含了天体光谱数据。\n",
    "返回值:四个浮点数，分别是：\n",
    "\n",
    "    B_fact_min: 所有考察光谱中，蓝端波长的最小值。\n",
    "    B_fact_max: 所有考察光谱中，蓝端波长的最大值。\n",
    "    R_fact_min: 所有考察光谱中，红端波长的最小值。\n",
    "    R_fact_max: 所有考察光谱中，红端波长的最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89cbe06-01bc-45f0-9577-cb194674aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于分析波长数据\n",
    "def wave_analyse(DATASET_PATH, FITS_PATH):\n",
    "    # 蓝端波长范围的最小和最大值\n",
    "    B_wave_min = np.log10(3800)\n",
    "    B_wave_max = np.log10(5700)\n",
    "\n",
    "    # 红端波长范围的最小和最大值\n",
    "    R_wave_min = np.log10(5900)\n",
    "    R_wave_max = np.log10(8800)\n",
    "    \n",
    "    # 初始化存储蓝端和红端波长数据的列表\n",
    "    B_wave = []\n",
    "    R_wave = []\n",
    "    label = []\n",
    "    \n",
    "    # 读取数据集路径下的CSV文件\n",
    "    data = pd.read_csv(DATASET_PATH)  \n",
    "    \n",
    "    # 获取所有FITS文件的路径\n",
    "    file_names = all_path(FITS_PATH)\n",
    "    print(len(file_names))\n",
    "    file_names = np.array(file_names)\n",
    "    \n",
    "    # 遍历每个FITS文件\n",
    "    for i in tqdm(range(len(file_names))):\n",
    "        try:  \n",
    "            # 打开FITS文件\n",
    "            f = fits.open(FITS_PATH + file_names[i])\n",
    "            try:\n",
    "                # 尝试获取第一个HDU的流量和波长数据\n",
    "                F = f[0].data[0]  # 流量\n",
    "                W = f[0].data[2]  # 波长\n",
    "            except:\n",
    "                # 如果第一个HDU中没有数据，则尝试从第二个HDU中获取\n",
    "                F = f[1].data[\"FLUX\"][0]\n",
    "                W = f[1].data[\"WAVELENGTH\"][0]\n",
    "                \n",
    "            # 获取红移值，并调整波长数据\n",
    "            z = f[0].header['Z']  # 红移\n",
    "            W = np.log10(W) - np.log10(1 + np.float64(z))\n",
    "            # 根据蓝端和红端的波长范围，获取相应的波长数据\n",
    "            B_W = (W > B_wave_min)*(W < B_wave_max)\n",
    "            R_W = (W > R_wave_min)*(W < R_wave_max)\n",
    "            # 添加蓝端波长数据\n",
    "            B_wave.append(W[B_W])\n",
    "            # 添加红端波长数据\n",
    "            R_wave.append(W[R_W])\n",
    "        except:\n",
    "            # 处理异常情况，跳过当前循环\n",
    "            print(\"Handling exceptions, skipped, indexed as : \", i)\n",
    "            continue\n",
    "    \n",
    "    # 获取所有蓝端波长数据的最大和最小值\n",
    "    B_fact_max = np.max(B_wave[0])\n",
    "    B_fact_min = np.min(B_wave[0])\n",
    "    for W in B_wave:\n",
    "        try:\n",
    "            B_fact_max = min(B_fact_max, np.max(W))\n",
    "            B_fact_min = max(B_fact_min, np.min(W))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 获取所有红端波长数据的最大和最小值\n",
    "    R_fact_max = np.max(R_wave[0])\n",
    "    R_fact_min = np.min(R_wave[0])\n",
    "    for W in R_wave:\n",
    "        try:\n",
    "            R_fact_max = min(R_fact_max, np.max(W))\n",
    "            R_fact_min = max(R_fact_min, np.min(W))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "\n",
    "    B_wave_scope = [B_fact_min, B_fact_max]\n",
    "    R_wave_scope = [R_fact_min, R_fact_max]\n",
    "    # 返回蓝端和红端波长的最小和最大值\n",
    "    return B_wave_scope, R_wave_scope\n",
    "\n",
    "# # 测试\n",
    "# B_wave_scope, R_wave_scope = wave_analyse(DATASET_PATH, FITS_PATH)\n",
    "# print(B_wave_scope, R_wave_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dc2718-36cb-4219-92f8-2583673773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于分析波长数据（去除了基于红移的波长矫正）\n",
    "def wave_analyse(DATASET_PATH, FITS_PATH):\n",
    "    # 蓝端波长范围的最小和最大值\n",
    "    B_wave_min = np.log10(3800)\n",
    "    B_wave_max = np.log10(5700)\n",
    "\n",
    "    # 红端波长范围的最小和最大值\n",
    "    R_wave_min = np.log10(5900)\n",
    "    R_wave_max = np.log10(8800)\n",
    "    \n",
    "    # 初始化存储蓝端和红端波长数据的列表\n",
    "    B_wave = []\n",
    "    R_wave = []\n",
    "    label = []\n",
    "    \n",
    "    # # 读取数据集路径下的CSV文件\n",
    "    # data = pd.read_csv(DATASET_PATH)  \n",
    "    \n",
    "    # 获取所有FITS文件的路径\n",
    "    file_names = all_path(FITS_PATH)\n",
    "    print(len(file_names))\n",
    "    file_names = np.array(file_names)\n",
    "    \n",
    "    # 遍历每个FITS文件\n",
    "    for i in tqdm(range(len(file_names))):\n",
    "        try:  \n",
    "            # 打开FITS文件\n",
    "            f = fits.open(FITS_PATH + file_names[i])\n",
    "            try:\n",
    "                # 尝试获取第一个HDU的流量和波长数据\n",
    "                F = f[0].data[0]  # 流量\n",
    "                W = f[0].data[2]  # 波长\n",
    "            except:\n",
    "                # 如果第一个HDU中没有数据，则尝试从第二个HDU中获取\n",
    "                F = f[1].data[\"FLUX\"][0]\n",
    "                W = f[1].data[\"WAVELENGTH\"][0]\n",
    "                \n",
    "            # 获取红移值，并调整波长数据\n",
    "            # z = f[0].header['Z']  # 红移\n",
    "            # W = np.log10(W) - np.log10(1 + np.float64(z))\n",
    "            W = np.log10(W)\n",
    "            \n",
    "            \n",
    "            # 根据蓝端和红端的波长范围，获取相应的波长数据\n",
    "            B_W = (W > B_wave_min)*(W < B_wave_max)\n",
    "            R_W = (W > R_wave_min)*(W < R_wave_max)\n",
    "            # 添加蓝端波长数据\n",
    "            B_wave.append(W[B_W])\n",
    "            # 添加红端波长数据\n",
    "            R_wave.append(W[R_W])\n",
    "        except:\n",
    "            # 处理异常情况，跳过当前循环\n",
    "            print(\"Handling exceptions, skipped, indexed as : \", i)\n",
    "            continue\n",
    "    \n",
    "    # 获取所有蓝端波长数据的最大和最小值\n",
    "    B_fact_max = np.max(B_wave[0])\n",
    "    B_fact_min = np.min(B_wave[0])\n",
    "    for W in B_wave:\n",
    "        try:\n",
    "            B_fact_max = min(B_fact_max, np.max(W))\n",
    "            B_fact_min = max(B_fact_min, np.min(W))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 获取所有红端波长数据的最大和最小值\n",
    "    R_fact_max = np.max(R_wave[0])\n",
    "    R_fact_min = np.min(R_wave[0])\n",
    "    for W in R_wave:\n",
    "        try:\n",
    "            R_fact_max = min(R_fact_max, np.max(W))\n",
    "            R_fact_min = max(R_fact_min, np.min(W))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "\n",
    "    B_wave_scope = [B_fact_min, B_fact_max]\n",
    "    R_wave_scope = [R_fact_min, R_fact_max]\n",
    "    # 返回蓝端和红端波长的最小和最大值\n",
    "    return B_wave_scope, R_wave_scope\n",
    "\n",
    "\n",
    "# DATASET_PATH = r\"./download/snrg_split_match_catalog/match_low_0_10.csv\"\n",
    "# FITS_PATH = r'./download/FITSDATA/test/'\n",
    "# B_wave_scope, R_wave_scope = wave_analyse(DATASET_PATH, FITS_PATH)\n",
    "# print(B_wave_scope, R_wave_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a14fb7-ac10-4475-a346-e407b8c4d0bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. 线性插值 (linear_inter)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd198f0d-3286-4c0a-b6a8-e5be97c2b0aa",
   "metadata": {},
   "source": [
    "功能描述\n",
    "linear_inter 函数用于对来自 FITS 文件的天体观测数据进行线性插值处理。该函数特别关注于蓝端和红端波段的光谱数据，计算这些波段在指定波长范围内的流量值。此操作对于统一不同观测数据的波长尺度，以便进行进一步分析。\n",
    "\n",
    "参数\n",
    "DATASET_PATH (str): 指向包含光谱数据概览的 CSV 文件的路径。尽管该参数在函数体中被读取，但在提供的代码示例中并未直接使用。\n",
    "FITS_PATH (str): 包含 FITS 文件的目录路径。这些文件中包含了用于分析的天体光谱数据。\n",
    "\n",
    "返回值\n",
    "函数返回三个对象：\n",
    "\n",
    "BR_flux_data (list): 一个列表，包含所有处理过的 FITS 文件中蓝端和红端波段的流量数据。每个元素是对应于一个文件中波段的流量值数组。\n",
    "B_wave_fixed (numpy.ndarray): 蓝端波长的固定采样数组，用于插值。\n",
    "R_wave_fixed (numpy.ndarray): 红端波长的固定采样数组，用于插值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74853d69-0a85-429c-8a0c-c2e000718928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数进行线性插值（去除了基于红移的波长矫正）\n",
    "def linear_inter(FITS_PATH, B_wave_scope, R_wave_scope,start=0,end=float('inf')):\n",
    "                               \n",
    "    # 计算蓝端和红端波长范围的对数值\n",
    "    B_wave_min = np.log10(3800)\n",
    "    B_wave_max = np.log10(5700)\n",
    "\n",
    "    R_wave_min = np.log10(5900)\n",
    "    R_wave_max = np.log10(8800)\n",
    "    \n",
    "    \n",
    "    # 初始化存储蓝红波段流量数据的列表\n",
    "    BR_flux_data = []\n",
    "    # 初始化文件名列表\n",
    "    file_names = []\n",
    "    \n",
    "    # 获取FITS文件的路径\n",
    "    file_names = all_path(FITS_PATH)\n",
    "    \n",
    "    # 初始化错误列表\n",
    "    error_list = []\n",
    "    \n",
    "    #定义遍历结束位置\n",
    "    end = min(len(file_names), end)\n",
    "    \n",
    "    # 遍历文件名列表\n",
    "    for i in tqdm(range(start,end)):\n",
    "        try:\n",
    "            # 打开FITS文件\n",
    "            f = fits.open(FITS_PATH + file_names[i])\n",
    "            \n",
    "            try:\n",
    "                # 尝试从第一个HDU获取流量和波长数据\n",
    "                F = f[0].data[0] \n",
    "                W = f[0].data[2] \n",
    "            except:\n",
    "                # 如果第一个HDU中没有数据，则从第二个HDU获取\n",
    "                F = f[1].data[\"FLUX\"][0]\n",
    "                W = f[1].data[\"WAVELENGTH\"][0]\n",
    "            \n",
    "            # # 获取红移值并调整波长数据\n",
    "            # z = f[0].header['Z']  \n",
    "            # W = np.log10(W) - np.log10(1 + np.float64(z))\n",
    "            W = np.log10(W)\n",
    "        \n",
    "            # # 定义蓝端和红端波长范围\n",
    "            # B_wave_scope = [B_fact_min, B_fact_max]\n",
    "            # R_wave_scope = [R_fact_min, R_fact_max]\n",
    "        \n",
    "            # 获取蓝端的流量和波长数据\n",
    "            B_flux = F[(W > B_wave_min)*(W < B_wave_max)]\n",
    "            B_wave = W[(W > B_wave_min)*(W < B_wave_max)]\n",
    "        \n",
    "            # # 对全部数据进行线性插值\n",
    "            # f = interp1d(W, F, kind = 'linear')\n",
    "        \n",
    "            # 获取红端的流量和波长数据\n",
    "            R_flux = F[(W > R_wave_min)*(W < R_wave_max)]\n",
    "            R_wave = W[(W > R_wave_min)*(W < R_wave_max)]\n",
    "              \n",
    "            # 分别对蓝端和红端的数据进行线性插值\n",
    "            B_f = interp1d(B_wave, B_flux, kind = 'linear')\n",
    "            R_f = interp1d(R_wave, R_flux, kind = 'linear')\n",
    "        \n",
    "            # 设置采样率为0.0001，并生成固定波长范围的蓝端和红端波长数组\n",
    "            B_wave_fixed = np.arange(B_wave_scope[0], B_wave_scope[1], 0.0001)\n",
    "            R_wave_fixed = np.arange(R_wave_scope[0], R_wave_scope[1], 0.0001)\n",
    "        \n",
    "            # 将插值后的蓝端和红端流量数据合并，并添加到列表中\n",
    "            BR_flux_data.append(np.concatenate((B_f(B_wave_fixed), R_f(R_wave_fixed))))\n",
    "        \n",
    "        except:\n",
    "            # 处理异常情况，跳过当前循环\n",
    "            print(\"Handling exceptions, skipped, indexed as : \", i) \n",
    "            error_list.append(i)\n",
    "            continue\n",
    "    \n",
    "    # 打印处理后的数据量和数据维度信息\n",
    "    print(\"The magnitude of the traffic data on the common wavelength position obtained \\\n",
    "          after interpolation of the red and blue bands separately：\", np.shape(BR_flux_data)[1])\n",
    "    print(\"Number of processed data：\", np.shape(BR_flux_data)[0])\n",
    "        \n",
    "    # 返回插值后的流量数据以及蓝端和红端的固定波长数组\n",
    "    return BR_flux_data, B_wave_fixed, R_wave_fixed, error_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d22274-3680-4a8c-be9d-509920e4ee1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. 连续谱拟合和归一化处理 (normalization_by_polyfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b9048-61e1-4297-be8f-3cdd02b943e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### csp_polyfit：  多项式拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e6139-e866-4496-8f78-c4e548677009",
   "metadata": {},
   "source": [
    "csp_polyfit：归一化光谱和波长，拟合出一个多项式\n",
    "\n",
    "参数说明：\n",
    "\n",
    "    sp: 光流量数据，一维数组。\n",
    "    angs: 角度数据，一维数组，应与光流量数据在长度上匹配。\n",
    "    param：\n",
    "        poly_global_order: 多项式拟合的全局阶数。\n",
    "        poly_upperlimit和poly_lowerlimit: 用于数据点选择的标准差倍数阈值。\n",
    "        poly_del_filled: 指示如何处理不符合条件的数据点，1 表示删除，其它值表示用拟合值填充。\n",
    "    \n",
    "返回值：\n",
    "\n",
    "    param：\n",
    "        poly_sp_c、poly_sp_s、poly_angs_c和poly_angs_s: 分别保存流量和波长的平均值、标准差\n",
    "        poly_P_g: 拟合过程中计算的多项式系数。\n",
    "        poly_sp_filtered和poly_angs_filtered: 过滤后的光流量和角度数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f2f86b-a823-496e-9832-f23a1ebf5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_polyfit(sp, angs, param):\n",
    "    # 标准化光流量\n",
    "    sp_c = np.mean(sp)   # 计算光流量的平均值，即中位流量\n",
    "    sp = sp - sp_c           # 以平均值为中心调整光流量，实现光流量的中心化\n",
    "    sp_s = np.std(sp)     # 计算光流量的标准差，即流量均方差\n",
    "    sp = sp / sp_s        # 用标准差标准化光流量\n",
    "    \n",
    "    # 标准化角度\n",
    "    angs_c = np.mean(angs)  # 计算波长的平均值\n",
    "    angs = angs - angs_c    # 以平均值为中心调整角度\n",
    "    angs_s = np.std(angs)   # 计算波长的标准差\n",
    "    angs = angs / angs_s    # 用标准差标准化波长\n",
    "    \n",
    "    # 更新参数字典\n",
    "    param['poly_sp_c'] = sp_c\n",
    "    param['poly_sp_s'] = sp_s\n",
    "    param['poly_angs_c'] = angs_c\n",
    "    param['poly_angs_s'] = angs_s\n",
    "    \n",
    "    # 初始化数据标志\n",
    "    data_flag = np.full(sp.shape, 1)\n",
    "    \n",
    "    i = 0\n",
    "    con = True\n",
    "    while(con):\n",
    "        # 使用多项式拟合并获取系数\n",
    "        P_g = np.polyfit(angs, sp, param['poly_global_order'])  # 计算多项式系数\n",
    "        param['poly_P_g'] = P_g\n",
    "        fitval_1 = np.polyval(P_g, angs)   # 计算多项式的拟合值\n",
    "        dev = fitval_1 - sp  # 计算拟合值与实际值的偏差\n",
    "        sig_g = np.std(dev)  # 计算偏差的标准差\n",
    "        \n",
    "        # 更新数据标志，用于识别需要保留的数据点\n",
    "        data_flag_new = (dev > (-param['poly_upperlimit'] * sig_g)) * (dev < (param['poly_lowerlimit'] * sig_g))\n",
    "    \n",
    "        if sum(abs(data_flag_new - data_flag)) > 0:\n",
    "            if param['poly_del_filled'] == 1: \n",
    "                data_flag = data_flag_new  # 更新数据标志\n",
    "            else:\n",
    "                fill_flag = data_flag - data_flag_new  # 计算需要填充的数据标志\n",
    "                index_1 = np.where(fill_flag != 0)  # 找出需要填充的数据点\n",
    "                sp[index_1] = fitval_1[index_1]  # 用拟合值填充这些数据点\n",
    "        else:\n",
    "            con = False  # 如果数据标志没有变化，则停止迭代\n",
    "        i += 1\n",
    "    #     print(\"iteration is %d\" %i)  # 打印迭代次数，此行为调试用，通常被注释\n",
    "    \n",
    "    # 根据最终的数据标志筛选数据\n",
    "    index_2 = np.where(data_flag != 0)\n",
    "    param['poly_sp_filtered'] = sp[index_2]  # 更新参数字典，保存过滤后的光流量数据\n",
    "    param['poly_angs_filtered'] = angs[index_2]  # 更新参数字典，保存过滤后的角度数据\n",
    "    \n",
    "    return param  # 返回更新后的参数字典\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b7a0a-4ae2-49af-b1f0-ba419960015e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### sp_median_polyfit1stage： 连续谱拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8aec77-d4ae-4f3f-91a6-2eb6d0b175e2",
   "metadata": {},
   "source": [
    "对一条光谱进行一系列处理最终获得一个拟合连续谱。具体操作包括：1.用中值滤波结果预剔除异常数据点 2.拟合一个归一化后的多项式 3.反归一化得到最终拟合连续谱\n",
    "\n",
    "使用说明\n",
    "\n",
    "    sp_median_polyfit1stage(flux, lambda_log, param)\n",
    "参数\n",
    "\n",
    "    flux: 光通量数据，一维数组。\n",
    "    lambda_log: 对数波长数据，与光通量数据相对应的一维数组。\n",
    "    param: 参数字典，包含处理过程中需要的各种参数。\n",
    "参数字典param的关键字段\n",
    "\n",
    "    median_radius: 中值滤波的窗口半径。\n",
    "    poly_lowerlimit: 多项式拟合预剔除下限，乘以标准差作为阈值。\n",
    "    poly_upperlimit: 多项式拟合预剔除上限，乘以标准差作为阈值。\n",
    "    poly_del_filled: 处理标记为填充的数据点的方法（1表示删除这些点，2表示用中值滤波结果填充）。\n",
    "    poly_angs_c, poly_angs_s: 波长预处理中的中心化和缩放参数。\n",
    "    poly_P_g: 连续谱拟合的多项式系数。\n",
    "    poly_sp_s, poly_sp_c: 连续谱拟合后的缩放和偏移参数。\n",
    "    poly_SM: 是否使用原始波长（1）或对数波长（其它值）。\n",
    "返回值\n",
    "\n",
    "    continum_fitted: 拟合的连续谱，一维数组，与输入的光通量数据对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4391d15b-e9f6-4bb4-91ae-b36e52499966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_median_polyfit1stage(flux, lambda_log,param):\n",
    "    flux1 = flux  # 复制光通量数据\n",
    "    lambda1 = lambda_log  # 复制对数波长数据\n",
    "\n",
    "    # 中值滤波\n",
    "    flux_median1 = medfilt(flux1, param['median_radius'])  # 使用中值滤波处理光通量数据\n",
    "\n",
    "    # 利用中值滤波结果进行预剔除\n",
    "    dev1 = flux_median1 - flux1  # 计算中值滤波后的光通量与原光通量的差值\n",
    "    sigma = np.std(dev1)  # 计算差值的标准差\n",
    "    # 生成数据标记，用于识别符合预剔除条件的数据点\n",
    "    data_flag1 = (dev1 < (param['poly_lowerlimit'] * sigma)) * (dev1 > (-param['poly_upperlimit'] * sigma))\n",
    "    \n",
    "    # 利用中值滤波结果进行伪光谱线的预剔除\n",
    "    fill_flag1 = 1 - data_flag1  # 生成填充标记，用于识别需要被填充的数据点\n",
    "    \n",
    "    # 根据参数决定如何处理填充标记指示的数据点\n",
    "    if param['poly_del_filled'] == 1:\n",
    "        index_1 = np.where(data_flag1)  # 找到符合条件的数据点索引\n",
    "        flux1 = flux1[index_1]  # 仅保留这些数据点的光通量数据\n",
    "        lambda1 = lambda1[index_1]  # 仅保留这些数据点的波长数据\n",
    "    elif param['poly_del_filled'] == 2:\n",
    "        index_2 = np.where(fill_flag1)  # 找到需要填充的数据点索引\n",
    "        flux1[index_2] = flux_median1[index_2]  # 用中值滤波结果填充这些数据点的光通量\n",
    "        \n",
    "    # 迭代拟合连续谱\n",
    "    param = csp_polyfit(flux1, lambda1, param)  # 执行连续谱的多项式拟合\n",
    "    \n",
    "    # 波长预处理\n",
    "    angs = lambda1 - param['poly_angs_c']  # 波长中心化\n",
    "    angs = angs / param['poly_angs_s']  # 波长缩放\n",
    "    \n",
    "    # 连续谱样本\n",
    "    fitval_g = np.polyval(param['poly_P_g'], angs)  # 使用多项式系数计算连续谱的拟合值\n",
    "    continum_fitted = fitval_g * param['poly_sp_s'] + param['poly_sp_c']  # 根据参数调整连续谱的拟合值\n",
    "    \n",
    "    # 根据参数选择使用原始波长或对数波长\n",
    "    if param['poly_SM'] == 1: \n",
    "        angss = lambda1\n",
    "    else: \n",
    "        angss = 10 ** lambda1  # 如果需要，将波长转换为对数形式\n",
    " \n",
    "    return continum_fitted  # 返回拟合的连续谱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342396c-95d3-45bf-80f6-42207675041c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### normalization_by_polyfit： 使用拟合连续谱对原始光谱数据归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d0c50-3b7a-45f4-8c21-6a091ad65c82",
   "metadata": {},
   "source": [
    "概述\n",
    "对每条光谱的蓝端和红端分别进行连续谱拟合和归一化。先分别处理蓝端和红端的光谱，对每一部分分别进行连续谱拟合，最后利用拟合结果对原始光谱数据进行归一化处理。\n",
    "\n",
    "参数说明\n",
    "\n",
    "    BR_flux_data: 原始光谱数据数组，假设为二维数组，其中每一行代表一条光谱数据。\n",
    "    B_wave_fixed: 蓝端波长的固定采样数组，一维数组。\n",
    "    R_wave_fixed: 红端波长的固定采样数组，一维数组。\n",
    "    param: (可选) 拟合参数字典，用于控制连续谱拟合的具体行为。如果未提供，则使用默认参数。\n",
    "    SAVE_PATH: (可选) 字符串，表示保存处理后数据的文件路径。如果提供此参数，函数将处理后的数据保存为NumPy文件。\n",
    "返回值\n",
    "\n",
    "    BR_flux_data_fitted: 归一化后的光谱数据数组，与原始光谱数据具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5495f758-458e-456e-8a4d-d68d22f3b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_by_polyfit(BR_flux_data, B_wave_fixed, R_wave_fixed, param=None, SAVE_PATH=None):\n",
    "    \"\"\"                      \n",
    "    对光谱数据进行连续谱拟合和归一化处理。\n",
    "\n",
    "    参数:\n",
    "    - BR_flux_data: 原始光谱数据数组。\n",
    "    - B_wave_fixed: 蓝端波长的固定采样数组。\n",
    "    - R_wave_fixed: 红端波长的固定采样数组。\n",
    "    - param: 拟合参数字典，可选。\n",
    "    - SAVE_PATH: 保存处理后数据的文件路径，可选。\n",
    "    \"\"\"\n",
    "    # 检查是否提供了拟合参数，如果没有，则设置默认参数\n",
    "    if param is None:\n",
    "        param = {'poly_global_order': 5, 'nor': 1, 'poly_lowerlimit': 3, 'poly_upperlimit': 4, \n",
    "                 'median_radius': 3, 'poly_SM': 0, 'poly_del_filled': 2}\n",
    "\n",
    "    # 初始化一个与原始光谱数据相同形状的数组，用于存储拟合后的数据\n",
    "    BR_flux_data_fitted = np.zeros_like(BR_flux_data)\n",
    "    \n",
    "    # 遍历每一行光谱数据\n",
    "    for i in tqdm(range(BR_flux_data_fitted.shape[0])):\n",
    "        \n",
    "        try:\n",
    "            # 对蓝端和红端波长的光谱数据分别进行连续谱拟合\n",
    "            B_continum_fitted_train = sp_median_polyfit1stage(BR_flux_data[i][:len(B_wave_fixed)], np.log10(B_wave_fixed), param)\n",
    "            R_continum_fitted_train = sp_median_polyfit1stage(BR_flux_data[i][len(B_wave_fixed):], np.log10(R_wave_fixed), param)\n",
    "\n",
    "            # 使用拟合得到的连续谱对原始光谱数据进行归一化\n",
    "            BR_flux_data_fitted[i, :len(B_wave_fixed)] = BR_flux_data[i][:len(B_wave_fixed)] / B_continum_fitted_train\n",
    "            BR_flux_data_fitted[i, len(B_wave_fixed):] = BR_flux_data[i][len(B_wave_fixed):] / R_continum_fitted_train\n",
    "        except:\n",
    "            print(f\"Handling exceptions, indexed as: {i}\")\n",
    "            continue\n",
    "\n",
    "    # 如果提供了保存路径，将处理后的数据保存到指定文件\n",
    "    if SAVE_PATH:\n",
    "        np.save(SAVE_PATH, BR_flux_data_fitted)\n",
    "    \n",
    "    return BR_flux_data_fitted\n",
    "\n",
    "# 注意: sp_median_polyfit1stage 需要根据实际的函数定义和参数进行调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643217d-8368-4e89-97bd-ac57ddec32ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. 用3σ原则处理异常值和标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38c8bb-8187-4fb5-87dd-5d897389516b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### three_sigma：异常值处理子函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220c3cd-0422-4616-b763-72974d45f0dd",
   "metadata": {},
   "source": [
    "函数基于3σ原则（三西格玛规则）对光谱数据中的异常值进行识别和处理。先假设数据遵循正态分布，数据集中在均值的±3σ（标准差）区间内的概率为99.73%，因此位于这个区间之外的点被视为异常值。该函数计算给定光谱数据片段的平均值和标准差，识别异常值，并将这些异常值替换为平均值，最后返回处理后的光谱数据点列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb1d497-1a73-4649-9cdd-9841063bae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sigma(Flux, flux_index, sigma_rate=3):\n",
    "    mean_value = Flux[flux_index].mean()  # 计算指定索引光谱数据的平均值\n",
    "    std_value = Flux[flux_index].std()  # 计算指定索引光谱数据的标准差\n",
    "    # 根据3σ原则确定异常值规则：异常值为超出平均值±3倍标准差的数据点\n",
    "    rule = (mean_value - sigma_rate * std_value > Flux[flux_index]) | (mean_value + sigma_rate * std_value < Flux[flux_index])\n",
    "    indice = np.arange(Flux[flux_index].shape[0])[rule]  # 获取所有异常值的索引位置\n",
    "    Flux_points = []  # 初始化一个列表，用于存放处理后的光谱数据点\n",
    "    for i in range(Flux[flux_index].shape[0]):  # 遍历光谱数据点\n",
    "        if i in indice:  # 如果当前点是异常值\n",
    "            Flux_points.append(mean_value)  # 用平均值替换异常值\n",
    "        else:\n",
    "            Flux_points.append(Flux[flux_index][i])  # 正常值保持不变\n",
    "    return Flux_points  # 返回处理后的光谱数据点列表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abc27f-6275-4bfd-b6e9-0c5f9601dd15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### three_sigma_processing："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd13405-1d19-4d0f-88ea-81fe1ef38405",
   "metadata": {},
   "source": [
    "用于批量处理光谱数据集，应用3σ原则去除异常值并进行后续的标准化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e858d64e-2a1c-4f0c-b9f9-5516b77dadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sigma_processing(Flux, SAVE_PATH_3sigma=None):\n",
    "    Flux_3sigma = []  # 初始化一个列表，用于存储所有光谱数据的3σ处理结果\n",
    "    for i in tqdm(range(Flux.shape[0])):  # 遍历所有光谱数据\n",
    "        Flux_3sigma.append(three_sigma(Flux, i, sigma_rate=3))  # 对每条光谱数据应用3σ处理\n",
    "    Flux_3sigma = np.array(Flux_3sigma)  # 将处理结果列表转换为NumPy数组\n",
    "    sc = StandardScaler()  # 创建一个StandardScaler对象，用于后续的标准化处理\n",
    "    Flux_3sigma_sc_T = sc.fit_transform(Flux_3sigma.T)  # 对3σ处理后的光谱数据进行标准化\n",
    "    Flux_3sigma_sc = Flux_3sigma_sc_T.T  # 转置回原来的形状\n",
    "    if SAVE_PATH_3sigma:\n",
    "        np.save(SAVE_PATH_3sigma, Flux_3sigma_sc)  # 将标准化后的光谱数据保存到指定路径\n",
    "    return  Flux_3sigma_sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef0fcd-d762-42bf-98fc-5c74ffcf8fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.提取info文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba588c47-67f4-463b-a1e9-f8fa6fc11f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fits_info(FITS_PATH, start=0, end = float('inf'), failed_indice=[],save_path=None):\n",
    "    \"\"\"\n",
    "    提取指定范围内的FITS文件中的OBSID、SNRG、RA、DEC字段，并存储到一个DataFrame中。\n",
    "    \n",
    "    参数：\n",
    "    FITS_PATH (str): FITS文件的目录路径。\n",
    "    start (int): 起始索引（包含）。\n",
    "    end (int): 结束索引（不包含）。\n",
    "    failed_indice (list): 要跳过的文件索引列表。\n",
    "    \n",
    "    返回：\n",
    "    df (pd.DataFrame): 包含提取信息的DataFrame，列名为obsid、snrg、ra、dec。\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # 获取FITS文件的路径\n",
    "    file_names = all_path(FITS_PATH)\n",
    "    \n",
    "    #定义遍历结束位置\n",
    "    end = min(len(file_names), end)\n",
    "\n",
    "    for i in tqdm(range(start,end)):\n",
    "        \n",
    "        # 检查当前索引是否在failed_indice中\n",
    "        if i in failed_indice:\n",
    "            continue\n",
    "        \n",
    "        fits_path = os.path.join(FITS_PATH, file_names[i])\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            header = hdul[0].header\n",
    "            obsid = header.get('OBSID', None)\n",
    "            snrg = header.get('SNRG', None)\n",
    "            ra = header.get('RA', None)\n",
    "            dec = header.get('DEC', None)\n",
    "            data.append([obsid, snrg, ra, dec])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['obsid', 'snrg', 'ra', 'dec'])\n",
    "    \n",
    "        # 如果提供了保存路径，则保存DataFrame到CSV文件\n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682f7c7-0183-4096-b3db-e075b5446862",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 预处理全流程打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5590d88b-2ab5-4fd5-b801-d222ae70e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_fits_files(FITS_PATH, SAVE_PATH = None, n=100000, wave_scope_path=\"./wave_scope.json\"):\n",
    "    \"\"\"\n",
    "    处理大量FITS文件的函数，执行数据插值、连续谱拟合、3σ处理和信息提取等操作。\n",
    "    \n",
    "    参数：\n",
    "    FITS_PATH (str): FITS文件的目录路径。\n",
    "    SAVE_PATH (str): 保存处理后数据的路径。\n",
    "    info_save_path (str): 保存提取信息的CSV文件路径。\n",
    "    n (int): 每次处理的最大文件数量。\n",
    "    wave_scope_path (str): 波长范围的JSON文件路径。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1.读取最小和最大值。保证和训练集处理流程一致\n",
    "    with open(wave_scope_path, \"r\") as file:\n",
    "        wave_scope = json.load(file)\n",
    "    B_wave_scope = wave_scope['B_wave_scope']\n",
    "    R_wave_scope = wave_scope['R_wave_scope']\n",
    "    \n",
    "    # 创建保存目录\n",
    "    flux_save_dir = os.path.join(SAVE_PATH, 'flux')\n",
    "    info_save_dir = os.path.join(SAVE_PATH, 'info')\n",
    "    os.makedirs(flux_save_dir, exist_ok=True)\n",
    "    os.makedirs(info_save_dir, exist_ok=True)\n",
    "    num_files = len(all_path(FITS_PATH)) // n + 1\n",
    "    \n",
    "    # 将一个大文件分多个部分处理\n",
    "    for i in range(num_files):\n",
    "        start = i * n\n",
    "        end = (i + 1) * n\n",
    "        \n",
    "         # 定义保存路径\n",
    "        flux_save_path = os.path.join(flux_save_dir, \"{}_flux.npy\".format(i))\n",
    "        info_save_path = os.path.join(info_save_dir, \"{}_info.csv\".format(i))\n",
    " \n",
    "        \n",
    "        # 2.在范围内进行插值，统一波长范围和数据格式\n",
    "        BR_flux_data, B_wave_fixed, R_wave_fixed, failed_indice = linear_inter(FITS_PATH, B_wave_scope, R_wave_scope, start=start, end=end)\n",
    "\n",
    "        # 3.进行连续谱拟合，并以此对原始光谱数据归一化\n",
    "        BR_flux_data_fitted = normalization_by_polyfit(BR_flux_data, B_wave_fixed, R_wave_fixed, param=None, SAVE_PATH=None)\n",
    "\n",
    "        # 4.基于3σ原则对光谱数据中的异常值进行识别和处理，并归一化，最后将数据保存到save_path\n",
    "        Flux_3sigma_sc = three_sigma_processing(BR_flux_data_fitted, flux_save_path)\n",
    "\n",
    "        # 5. 提取当前范围内的FITS文件信息（OBSID、SNRG、RA、DEC），并将信息保存到CSV文件中\n",
    "        info_df = extract_fits_info(FITS_PATH, start, end, failed_indice, info_save_path)\n",
    "        \n",
    "        print(\"flux shape:\",Flux_3sigma_sc.shape)\n",
    "        print(\"info shape:\",info_df.shape)\n",
    "\n",
    "        \n",
    "    return Flux_3sigma_sc, info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483208e-b1de-4fb2-a38c-015fa9b641b6",
   "metadata": {},
   "source": [
    "## 处理 5-6的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7ab917-6517-4ac2-a53a-cf3b8a3153ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [09:24<00:00, 177.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the traffic data on the common wavelength position obtained           after interpolation of the red and blue bands separately： 3450\n",
      "Number of processed data： 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [08:18<00:00, 200.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [12:38<00:00, 131.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [03:57<00:00, 421.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux shape: (100000, 3450)\n",
      "info shape: (100000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [12:12<00:00, 136.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the traffic data on the common wavelength position obtained           after interpolation of the red and blue bands separately： 3450\n",
      "Number of processed data： 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [09:02<00:00, 184.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [13:29<00:00, 123.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [03:50<00:00, 434.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux shape: (100000, 3450)\n",
      "info shape: (100000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [12:37<00:00, 132.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the traffic data on the common wavelength position obtained           after interpolation of the red and blue bands separately： 3450\n",
      "Number of processed data： 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [09:22<00:00, 177.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [12:34<00:00, 132.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [03:27<00:00, 482.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux shape: (100000, 3450)\n",
      "info shape: (100000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [11:04<00:00, 150.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the traffic data on the common wavelength position obtained           after interpolation of the red and blue bands separately： 3450\n",
      "Number of processed data： 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [08:57<00:00, 186.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [12:08<00:00, 137.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [03:32<00:00, 470.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux shape: (100000, 3450)\n",
      "info shape: (100000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14781/14781 [01:34<00:00, 156.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the traffic data on the common wavelength position obtained           after interpolation of the red and blue bands separately： 3450\n",
      "Number of processed data： 14781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14781/14781 [01:03<00:00, 233.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 14781/14781 [01:45<00:00, 140.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 14781/14781 [00:34<00:00, 434.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flux shape: (14781, 3450)\n",
      "info shape: (14781, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-3.65573745e+00,  6.61768690e-02,  2.41959391e+00, ...,\n",
       "         -3.97630355e-01, -1.13896112e-01,  1.72082406e-02],\n",
       "        [-2.84429900e+00,  6.76444016e-01,  1.53069553e+00, ...,\n",
       "         -3.50407837e-01, -4.34876549e-01, -4.31158671e-01],\n",
       "        [ 2.77642320e-02,  2.77642320e-02,  3.22832556e+00, ...,\n",
       "         -1.09846337e-03,  1.70970843e-01, -1.01080763e-01],\n",
       "        ...,\n",
       "        [-3.48395273e+00, -3.47886406e+00, -5.13026628e-01, ...,\n",
       "         -7.07390854e-01, -7.63387570e-01, -6.49535991e-01],\n",
       "        [-3.80726222e+00, -3.80813942e+00,  8.50497022e-01, ...,\n",
       "         -1.85162197e+00, -1.63996205e+00, -1.75842341e+00],\n",
       "        [-2.51505162e+00, -2.66513708e+00, -1.66494711e+00, ...,\n",
       "          1.61692139e-02,  1.41635171e+00,  1.61692139e-02]]),\n",
       "             obsid  snrg          ra        dec\n",
       " 0      1055007082  5.78   29.424651  18.950419\n",
       " 1      1055007085  6.00   29.774369  18.958642\n",
       " 2      1055007089  5.90   29.561938  19.021214\n",
       " 3      1055007104  5.26   29.257512  19.198055\n",
       " 4      1055007156  5.94   28.959849  18.875139\n",
       " ...           ...   ...         ...        ...\n",
       " 14776  1128113058  5.03  286.945645  34.701834\n",
       " 14777  1128114019  5.13  282.033744  36.062383\n",
       " 14778  1128114213  5.84  282.035833  35.587501\n",
       " 14779  1128114217  5.90  281.888908  35.492018\n",
       " 14780  1128116207  5.24  282.292021  36.674480\n",
       " \n",
       " [14781 rows x 4 columns])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FITS_PATH = r'../catalog_download/FITS/5-6/'\n",
    "SAVE_PATH = r'../Fits_preprocessed/5-6/'\n",
    "\n",
    "n = 100000   \n",
    "\n",
    "process_large_fits_files(FITS_PATH, SAVE_PATH = SAVE_PATH , n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc32f8-a4da-487e-9b51-b0f93a78f990",
   "metadata": {},
   "source": [
    "## 处理 16-20的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88452230-29ed-4979-9a1b-213e81204ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                      | 3644/50000 [00:25<05:38, 137.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▊                                                                      | 3820/50000 [00:26<05:17, 145.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  3797\n",
      "Handling exceptions, skipped, indexed as :  3826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▉                                                                      | 3878/50000 [00:27<05:43, 134.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  3854\n",
      "Handling exceptions, skipped, indexed as :  3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▉                                                                      | 3925/50000 [00:27<05:16, 145.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  3902\n",
      "Handling exceptions, skipped, indexed as :  3929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                      | 3974/50000 [00:27<05:08, 149.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  3957\n",
      "Handling exceptions, skipped, indexed as :  3981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                     | 4034/50000 [00:28<05:23, 142.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  4008\n",
      "Handling exceptions, skipped, indexed as :  4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                     | 4079/50000 [00:28<05:30, 139.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  4058\n",
      "Handling exceptions, skipped, indexed as :  4083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                     | 4136/50000 [00:29<05:32, 138.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  4109\n",
      "Handling exceptions, skipped, indexed as :  4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                     | 4178/50000 [00:29<05:33, 137.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▊                                                                    | 5102/50000 [00:36<05:34, 134.27it/s]"
     ]
    }
   ],
   "source": [
    "FITS_PATH = r'../catalog_download/FITS/16-20/'\n",
    "SAVE_PATH = r'../Fits_preprocessed/16-20/'\n",
    "\n",
    "n = 50000   \n",
    "\n",
    "process_large_fits_files(FITS_PATH, SAVE_PATH = SAVE_PATH , n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b4a80-79d5-4d06-9e39-94c45d82d4e9",
   "metadata": {},
   "source": [
    "## 处理 30-35的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7a0d5-9ca6-43e2-a174-83eac2dcb8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                       | 5165/100000 [00:26<08:19, 189.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling exceptions, skipped, indexed as :  5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                      | 6104/100000 [00:31<08:23, 186.32it/s]"
     ]
    }
   ],
   "source": [
    "FITS_PATH = r'../catalog_download/FITS/30-35/'\n",
    "SAVE_PATH = r'../Fits_preprocessed/30-35/'\n",
    "\n",
    "n = 50000   \n",
    "\n",
    "process_large_fits_files(FITS_PATH, SAVE_PATH = SAVE_PATH , n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7949b2-37bd-4d09-a534-c200244d85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40_50\n",
    "LABEL_PATH = './LABELS/40_50_old.npy'\n",
    "SAVE_PATH = \"./spectra_after_processing/3sigma/BR_Flux_Preprocessing_payne_40_50.npy\"\n",
    "\n",
    "labels_40_50 = np.load(LABEL_PATH, allow_pickle=True).squeeze()\n",
    "labels_40_50 = pd.DataFrame(labels_40_50, columns=columns)\n",
    "labels_40_50['snrg'] = 4\n",
    "Flux_40_50 = np.load(SAVE_PATH)\n",
    "Flux_40_50 = np.pad(Flux_40_50, (0, 1), 'edge')[:Flux_40_50.shape[0], :]\n",
    "\n",
    "print(Flux_40_50.shape)\n",
    "print(labels_40_50.shape)\n",
    "\n",
    "labels_40_50_tmp = np.zeros((len(labels_40_50),17))\n",
    "for i in range(len(labels_40_50)):  \n",
    "    labels_40_50_tmp[i,0] = labels_40_50['Teff[K]'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,1] = labels_40_50['Logg'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,2] = labels_40_50['CH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,3] = labels_40_50['NH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,4] = labels_40_50['OH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,5] = labels_40_50['MgH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,6] = labels_40_50['AlH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,7] = labels_40_50['SiH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,8] = labels_40_50['SH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,9] = labels_40_50['KH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,10] = labels_40_50['CaH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,11] = labels_40_50['TiH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,12] = labels_40_50['CrH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,13] = labels_40_50['MnH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,14] = labels_40_50['FeH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,15] = labels_40_50['NiH'][i][0].astype('float')\n",
    "    labels_40_50_tmp[i,16] = labels_40_50['snrg'][i].astype('float')\n",
    "    \n",
    "LABEL_PATH_new = './LABELS/40_50.npy'\n",
    "np.save(LABEL_PATH_new, labels_40_50_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d5eb5-6cab-4519-beff-42463d10d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Teff[K]', 'Logg', 'CH', 'NH', 'OH', 'MgH', 'AlH', 'SiH', 'SH', 'KH', 'CaH', 'TiH', 'CrH','MnH', 'FeH', 'NiH','snrg']\n",
    "\n",
    "for i in range(10):\n",
    "    # print(DATASET_PATH)\n",
    "    \n",
    "    DATASET_PATH = \"./download/snrg_split/match_{}_{}.csv\".format(i*10,i*10+10)\n",
    "    FITS_PATH = \"./FITSDATA/{}_{}/\".format(i*10,i*10+10)\n",
    "    LABEL_PATH = \"./data_after_processing/LABELS/{}_{}\".format(i*10,i*10+10)\n",
    "    SAVE_PATH = \"./data_after_processing/Flux/{}_{}.npy\".format(i*10,i*10+10)\n",
    "    \n",
    "    \n",
    "    # 从numpy文件加载标签数据，允许pickle对象\n",
    "    label = np.load(LABEL_PATH,  allow_pickle=True)\n",
    "    # 将加载的标签数据转换为pandas DataFrame，并指定列名\n",
    "    label = pd.DataFrame(label, columns=columns)\n",
    "   \n",
    "    # 打印光谱数据和标签数据的形状\n",
    "    print(Flux_5_8.shape)\n",
    "    print(labels_5_8.shape)\n",
    "\n",
    "    # 初始化一个临时numpy数组，用于存储处理后的标签数据，行数与labels_5_8相同，有17列\n",
    "    labels_5_8_tmp = np.zeros((len(labels_5_8),17))\n",
    "    # 遍历标签DataFrame的每一行\n",
    "    for i in range(len(labels_5_8)):  \n",
    "        # 对于每个化学元素和'snrg'，从DataFrame中提取它们的值，转换为float类型，然后存储到临时数组中\n",
    "        labels_5_8_tmp[i,0] = labels_5_8['Teff[K]'][i][0].astype('float')\n",
    "        labels_5_8_tmp[i,1] = labels_5_8['Logg'][i][0].astype('float')\n",
    "        # 继续提取和转换化学元素的值...\n",
    "        labels_5_8_tmp[i,16] = labels_5_8['snrg'][i].astype('float')\n",
    "\n",
    "    # 定义新标签数据的路径\n",
    "    LABEL_PATH_new = './LABELS/5_8.npy'\n",
    "    # 将处理后的标签数据保存到numpy文件\n",
    "    np.save(LABEL_PATH_new, labels_5_8_tmp)\n",
    "    \n",
    "    # 从numpy文件加载光谱数据\n",
    "    Flux_5_8 = np.load(SAVE_PATH)\n",
    "    # 对光谱数据进行边缘填充，然后裁剪回原始长度\n",
    "    Flux_5_8 = np.pad(Flux_5_8, (0, 2), 'edge')[:Flux_5_8.shape[0], :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
